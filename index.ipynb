{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assess Huntington's disease progression from PET/MR images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import glob\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import hd_classifier\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inflammation assesment using PBR scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participants information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "participants_df = pd.read_csv('data/participants.tsv', delimiter='\\t')\n",
    "participants_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also set some colors for the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subject_colors = {'HD': 'r', 'pre-HD': 'g', 'control': 'b'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis\n",
    "\n",
    "We extract all voxels for each subject and region directly from the nifti images and the masks generated from Freesurfer.\n",
    "\n",
    "We use principal component analysis to transform each subject into the vector space spanned by the eigenvectors of the covariance matrix. The plots show the components along each axis in this subspace for each patient. The axis are ordered according to the value of the eigenvectors.\n",
    "\n",
    "We choose a few statistical quantities to describe each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_1 = lambda x: np.percentile(x, q=25)\n",
    "q_3 = lambda x: np.percentile(x, q=75)\n",
    "\n",
    "features = {'value' : {'min' : np.min, 'max' : np.max, 'mean' : np.mean, \n",
    "                       'q_1' : q_1, 'median' : np.median, 'q_3' : q_3 }} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the voxel data, calculate statistical features for all regions and subjects, and put them into a `pd.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_tracer_data = './data/func'\n",
    "items = os.listdir(path_to_tracer_data)\n",
    "\n",
    "subjects = hd_classifier.make_subjects(items, path_to_tracer_data)\n",
    "image_filter = '^r.*\\.lin_T1_orientOK_skullstripped_norm_sm6mm.nii'\n",
    "\n",
    "masked_region_df_pbr, masked_region_features_pbr = hd_classifier.extract_features(subjects, features, image_filter)\n",
    "masked_region_features_pbr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the principal component rotated basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(whiten=True)\n",
    "pca.fit(masked_region_features_pbr.as_matrix())\n",
    "\n",
    "rotated_subjects_pbr = pca.fit_transform(masked_region_features_pbr.as_matrix())\n",
    "\n",
    "pca_results = pd.DataFrame(dict(first_feature=rotated_subjects_pbr[:,0],\n",
    "                         second_feature=rotated_subjects_pbr[:,1],\n",
    "                         subject_id = masked_region_features_pbr.index.values))\n",
    "\n",
    "pca_results.sort_values(by='first_feature', inplace=True)\n",
    "pca_results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "pca_results = pca_results.merge(participants_df[['subject_id', 'group']], on='subject_id')\n",
    "pca_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a list with all the name features as region_feature to label the plots. E.g. thalamus_min\n",
    "pca_features = [(item[1] + ' ' + item[0]).title() \n",
    "                for item in itertools.product(list(masked_region_features_pbr.columns.levels[1]),                         \n",
    "                                              list(masked_region_features_pbr.columns.levels[2]))]\n",
    "\n",
    "# Get the coefficient of each feature for the first 3 principal axis\n",
    "transformed_features = sorted(zip(pca_features, *pca.components_[0:4]), key=lambda t: t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's order the data according to the score in the plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masked_region_df_pbr['subject_id'] = pd.Categorical(masked_region_df_pbr['subject_id'], pca_results.subject_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a couple of features classify inflammation in patients vs controls. Even only one feature, classifies the subjects in at least two groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some values overlap in the final plot, so we add some ad-hoc jittering\n",
    "jitters = np.zeros(11)\n",
    "jitters[3] = -.03\n",
    "jitters[4] = .03\n",
    "\n",
    "joined = pca_results.join(pd.DataFrame(dict(jitters=jitters)))\n",
    "groups = joined.groupby('group')\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "outer_grid = gridspec.GridSpec(1, 2, wspace=0.1, width_ratios=[4,4])\n",
    "\n",
    "right_plot = gridspec.GridSpecFromSubplotSpec(2, 1,\n",
    "            subplot_spec=outer_grid[0], hspace=0, height_ratios=[1,3])\n",
    "\n",
    "marker_size = 350\n",
    "# plot the first and second components in a scatter plot\n",
    "ax = plt.Subplot(fig, right_plot[1])  \n",
    "for name, items in groups:\n",
    "    ax.scatter(items.first_feature, items.second_feature, s=marker_size, alpha=0.4, \n",
    "               c=subject_colors[name], label=name)\n",
    "\n",
    "for idx in range(len(pca_results.index)):\n",
    "    ax.text(pca_results.first_feature[idx], pca_results.second_feature[idx], str(idx+1), \n",
    "            horizontalalignment='center', verticalalignment='center')\n",
    "ax.set_xlabel('first PCA axis')\n",
    "ax.set_ylabel('second PCA axis')\n",
    "ax.legend(labelspacing=1.4)\n",
    "fig.add_subplot(ax)\n",
    "\n",
    "# plot the first component in a line on top\n",
    "ax_top = plt.Subplot(fig, right_plot[0])\n",
    "for name, items in groups:\n",
    "    ax_top.scatter(items.first_feature, items.jitters, s=marker_size, alpha=0.4, \n",
    "               color=subject_colors[name], label=name)\n",
    "\n",
    "ax_top.set_xticks([])\n",
    "ax_top.set_yticks([0])\n",
    "ax_top.set_yticklabels([])\n",
    "ax_top.set_ylabel('')\n",
    "ax_top.xaxis.set_label_coords(0.5, 0.88)\n",
    "ax_top.set_xlabel('first PCA axis')\n",
    "ax_top.set(xlim=ax.get_xlim())\n",
    "for sp in ax_top.spines.values(): sp.set_visible(False)\n",
    "fig.add_subplot(ax_top)\n",
    "\n",
    "# plot the eigenvectors in the original feature space\n",
    "number_of_components = 3\n",
    "number_of_features = len(transformed_features)\n",
    "\n",
    "left_plot = gridspec.GridSpecFromSubplotSpec(1, number_of_components,\n",
    "            subplot_spec=outer_grid[1], wspace=0.18)\n",
    "\n",
    "for i in range(number_of_components):\n",
    "    ax = plt.Subplot(fig, left_plot[i])  \n",
    "    ax.plot([item[i+1] if i != 0 else -item[i+1] for item in transformed_features], list(range(number_of_features)))\n",
    "    ax.set_title(round(pca.explained_variance_ratio_[i],2))\n",
    "    ax.set(xlim=(-.4, .4))\n",
    "    ax.set(ylim=(0, number_of_features-1))\n",
    "    ax.set_yticks(list(range(number_of_features)))\n",
    "    ax.set_yticklabels([item[0] for item in transformed_features] if i==2 else [])\n",
    "    ax.yaxis.tick_right()\n",
    "    ax.set_xticks([-0.4, -.2, 0, .2, 0.4])\n",
    "    ax.set_xticklabels([-0.4, -0.2, 0, 0.2, 0.4])\n",
    "    for i in range(0,4):\n",
    "        ax.axhline(y=5.5+6*i, ls='dashed', c='black', alpha=0.4)\n",
    "    \n",
    "    fig.add_subplot(ax)\n",
    "\n",
    "fig.savefig('results/figs/pca_analysis.pdf', format='pdf') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region of interest analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = masked_region_df_pbr.merge(pca_results[['group', 'subject_id', 'first_feature']], on='subject_id')\n",
    "merged.sort_values(by='first_feature', inplace=True)\n",
    "merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "regions = ['pallidum', 'putamen', 'caudate', 'thalamus']\n",
    "region_data = list(map(lambda r: merged[merged['region'] == r], regions))\n",
    "\n",
    "def plot_roi_histograms(data, ax, ax_hist):\n",
    "    sns.violinplot(data=data, x=\"subject_id\", y=\"value\", bw=.2, \n",
    "                   scale='count', cut=1, linewidth=1, ax=ax)\n",
    "    groups = data.groupby('group')\n",
    "    for name, group in groups:\n",
    "        sns.kdeplot(group['value'], vertical=True, ax=ax_hist,\n",
    "                    label=name, color=subject_colors[name],\n",
    "                    ls=('--' if name=='pre-HD' else '-'))\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# gridspec inside gridspec\n",
    "outer_grid = gridspec.GridSpec(2, 2, wspace=0.1, hspace=0.1)\n",
    "\n",
    "for i in range(4):\n",
    "    inner_grid = gridspec.GridSpecFromSubplotSpec(1, 2,\n",
    "            subplot_spec=outer_grid[i], wspace=0.0, hspace=0.0, width_ratios=[9,2],)\n",
    "    ax = plt.Subplot(fig, inner_grid[0])  \n",
    "\n",
    "    ax_hist = plt.Subplot(fig, inner_grid[1])\n",
    "    plot_roi_histograms(region_data[i], ax, ax_hist)\n",
    "    ax.set_title(regions[i])\n",
    "    ax.set(ylim=(masked_region_df_pbr['value'].min(), masked_region_df_pbr['value'].max()))\n",
    "    ax_hist.set(ylim=(masked_region_df_pbr['value'].min(), masked_region_df_pbr['value'].max()))\n",
    "\n",
    "    ax.set_yticks([0.6, 0.8, 1, 1.2, 1.4, 1.6])\n",
    "    # show only xticklabels only for the lower plots and show the patient number instead of subject_id\n",
    "    if i in [0,1]:\n",
    "        ax.set_xticks([])\n",
    "    else:\n",
    "        ax.set_xticklabels(list(range(1, len(merged)+1)))\n",
    "    ax_hist.set_xticks([])\n",
    "    ax_hist.set_yticks([])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    fig.add_subplot(ax)\n",
    "    fig.add_subplot(ax_hist)\n",
    "\n",
    "all_axes = fig.get_axes()\n",
    "\n",
    "#show only the outside spines\n",
    "for ax in all_axes:\n",
    "    for sp in ax.spines.values():\n",
    "        sp.set_visible(False)\n",
    "        \n",
    "plt.savefig('results/figs/regions_of_interest_pbr.pdf', format='pdf')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between caudate volumes and inflammation\n",
    "\n",
    "The volume of each region as it has a lot of variability, even among subjects with similar disease progression. We normalize the volumes and show the correlations between the value of the classifier for inflammation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "region_volumes = masked_region_df_pbr.groupby(['subject_id', 'region']).agg('count').unstack()\n",
    "# Pretty ugly fix: no time to do something smarter\n",
    "region_volumes.columns = [' '.join(col).strip().split(' ')[1] for col in region_volumes.columns.values]\n",
    "\n",
    "intracraneal_volume_df = participants_df[['subject_id', 'intracraneal_volume']].set_index('subject_id')\n",
    "\n",
    "merged = region_volumes.join(intracraneal_volume_df)\n",
    "merged = merged.div(merged.intracraneal_volume, axis='index')\n",
    "\n",
    "normalized_region_volumes = hd_classifier.normalize(merged)\n",
    "normalized_region_volumes.drop('intracraneal_volume', axis=1, inplace=True)\n",
    "\n",
    "to_fit = normalized_region_volumes.join(pca_results[['first_feature', 'subject_id', 'group']].set_index('subject_id'))\n",
    "to_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "import statsmodels.api as sm\n",
    "\n",
    "x = to_fit.first_feature\n",
    "y = to_fit['caudate'].values\n",
    "X = sm.add_constant(x)\n",
    "\n",
    "re = sm.OLS(y, X).fit()\n",
    "print(re.summary())\n",
    "\n",
    "st, data, ss2 = summary_table(re, alpha=0.05)\n",
    "\n",
    "fittedvalues = data[:,2]\n",
    "predict_mean_se  = data[:,3]\n",
    "predict_mean_ci_low, predict_mean_ci_upp = data[:,4:6].T\n",
    "predict_ci_low, predict_ci_upp = data[:,6:8].T\n",
    "\n",
    "plt.plot(x, fittedvalues, 'b-', lw=1)\n",
    "plt.plot(x, predict_ci_low, 'r--', lw=1.5)\n",
    "plt.plot(x, predict_ci_upp, 'r--', lw=1.5)\n",
    "plt.plot(x, predict_mean_ci_low, 'b--', lw=1)\n",
    "plt.plot(x, predict_mean_ci_upp, 'b--', lw=1)\n",
    "\n",
    "# You don't neeed this if you do the relative caudates\n",
    "#labels = [str(i+1) if i not in (3,4) else str('4,5') for i in range(len(x))]\n",
    "#labels[4] = ''\n",
    "#labels = [str(i+1) for i in range(len(x))]\n",
    "\n",
    "#plt.scatter(x[:3], y[:3], s=marker_size, alpha=0.4, c='b', label='control')\n",
    "#plt.scatter(x[3], y[3], s=marker_size, alpha=0.4, c='g', label='pre-HD')\n",
    "#plt.scatter(x[4:], y[4:], s=marker_size, alpha=0.4, c='r', label='HD')\n",
    "\n",
    "#for i in range(len(x)):\n",
    "#    plt.text(x[i], y[i], labels[i], \n",
    "#            horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "groups = to_fit.groupby('group')\n",
    "\n",
    "for name, items in groups:\n",
    "    plt.scatter(items.first_feature, items.caudate, s=marker_size, alpha=0.4, \n",
    "               c=subject_colors[name], label=name)\n",
    "\n",
    "for idx in range(len(to_fit)):\n",
    "    plt.text(x[idx], y[idx], str(idx+1), \n",
    "            horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.xlabel('first PCA axis')\n",
    "plt.ylabel('caudate volume')\n",
    "plt.legend(labelspacing=1.4)\n",
    "plt.savefig('results/figs/region_volumes_vs_inflammation_scores.pdf', format='pdf') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thalamic nuclei analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from nilearn.image import resample_to_img, new_img_like\n",
    "from nilearn.masking import apply_mask\n",
    "\n",
    "def merge_nuclei_masks(nuclei, nuclei_masks, imgs):\n",
    "    \n",
    "    #print(\"Making a mask with nuclei: \" + str(nuclei))\n",
    "    \n",
    "    masks_to_join = []\n",
    "    for k in nuclei:\n",
    "        resampled = resample_to_img(nuclei_masks[k], imgs[0])\n",
    "        masks_to_join.append(resampled.get_data() > 0.5)\n",
    "\n",
    "    joint_mask_data_as_bool = reduce(np.logical_or, masks_to_join) \n",
    "    joint_mask_as_bool = new_img_like(imgs[0], joint_mask_data_as_bool)\n",
    "    try: \n",
    "        masked_motor_nuclei = apply_mask(imgs, joint_mask_as_bool)\n",
    "        #return masked_motor_nuclei, np.sum(joint_mask_data_as_bool)\n",
    "        return masked_motor_nuclei\n",
    "    except:\n",
    "        raise Exception('bad thing')\n",
    "\n",
    "def find_threshold(imgs, q):\n",
    "    from math import floor\n",
    "    parts = []\n",
    "    for img in imgs:\n",
    "        threshold_index = floor(q * len(img))\n",
    "        parts.append(np.partition(img, threshold_index)[threshold_index:])\n",
    "    merged = np.concatenate(parts)\n",
    "    merged_threshold = floor (1- len(merged) / len(parts))\n",
    "    return np.partition(merged, merged_threshold)[merged_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the images in MNI space, instead of subject space, so we can use the atlas, and then apply all the masks in the atlas to each of the subject's MNI images. The result is a dictionary `masked_imgs` with the masked images for each nuclei and patient.\n",
    "\n",
    "We have stored the `masked_imgs` in a `.pickle` file, because the license of the Morel atlas does not allow redistribution. If you have a [licensed copy of the Morel atlas](http://www.lead-dbs.org/?page_id=45), you can generate the data yourself, using the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_masked_nuclei_imgs(imgs, path_to_morel_atlas, excludes):\n",
    "    \n",
    "    left_volumes = glob.glob(os.path.join(path_to_morel_atlas, 'left-vols-1mm/*.nii.gz'))\n",
    "    right_volumes = glob.glob(os.path.join(path_to_morel_atlas,'right-vols-1mm/*.nii.gz'))\n",
    "\n",
    "    def parse_nuclei_name(vol):\n",
    "        return os.path.dirname(vol).split('/')[-1].split('-')[0] + '_' + os.path.basename(vol).split('.')[0]\n",
    "\n",
    "    nuclei_mask_dict = { parse_nuclei_name(vol) : vol \n",
    "                         for vol in left_volumes + right_volumes \n",
    "                         if not ''.join(parse_nuclei_name(vol).split('_')[1:]).startswith(tuple(excludes)) }\n",
    "\n",
    "    nuclei_masks = { k: nib.load(v) for k, v in nuclei_mask_dict.items() }\n",
    "    masked_img = {}\n",
    "    nuclei_sizes = {}\n",
    "\n",
    "    for k, v in nuclei_masks.items():\n",
    "        resampled = resample_to_img(v, imgs[0])\n",
    "        resampled_data_as_bool = resampled.get_data() > 0.5\n",
    "        nuclei_sizes[k] = np.sum(resampled_data_as_bool)\n",
    "        resampled_as_bool = new_img_like(resampled, resampled_data_as_bool)\n",
    "        try: \n",
    "            masked_img[k] = apply_mask(imgs, resampled_as_bool)\n",
    "        except:\n",
    "            print('Something is wrong for nucleus ', k)\n",
    "            continue\n",
    "            \n",
    "    return masked_img, nuclei_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_tracer_data = './data/func'\n",
    "items = os.listdir(path_to_tracer_data)\n",
    "\n",
    "subjects = hd_classifier.make_subjects(items, path_to_tracer_data)\n",
    "mni_image_filter = 'nl_MNI152_norm_sm6mm.nii'\n",
    "\n",
    "subjects.sort(key=lambda s: pca_results[pca_results['subject_id']==s.subject_id].index.tolist()[0])\n",
    "\n",
    "assert ([s.subject_id for s in subjects] == list(pca_results['subject_id']))\n",
    "\n",
    "mni_images = map(lambda s: hd_classifier.find_masks(s.images, mni_image_filter), subjects)\n",
    "imgs = [nib.load(image) for image in list(itertools.chain.from_iterable(mni_images))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickled_masked_nuclei_imgs = './results/masked_nuclei_imgs.pickle'\n",
    "\n",
    "if os.path.isfile(pickled_masked_nuclei_imgs):\n",
    "    with open(pickled_masked_nuclei_imgs, 'rb') as f:\n",
    "        nuclei_masks = pickle.load(f)\n",
    "else:\n",
    "    path_to_morel_atlas = './data/private/Atlas/Morel'\n",
    "    excluded_nuclei = [] # = ['global', 'MAX']\n",
    "    masked_img = make_masked_nuclei_imgs(imgs, path_to_morel_atlas, excluded_nuclei)\n",
    "    \n",
    "    with open(pickled_masked_nuclei_imgs, 'wb') as f:\n",
    "        pickle.dump(masked_img, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram of uptake for each relevant nuclei and subject. For the remaining nuclei in the Morel Atlas differences between controls and patients are not as marked as for those selected. The vertical lines are a guide ot the eye and mark the value of SUVR corresponding to the 95 percentile of all voxels in the control group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relevant_nuclei_group_names = ['left_VLpv', 'left_PuL', 'left_VApc']\n",
    "relevant_nuclei_groups = { k: [it for it in list(nuclei_masks.keys()) if k in it] for k in relevant_nuclei_group_names }\n",
    "\n",
    "masked_relevant_groups = {k: merge_nuclei_masks(v, nuclei_masks, imgs) for k, v in relevant_nuclei_groups.items()}\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "# gridspec two split all patients and averages\n",
    "outer_grid = gridspec.GridSpec(1, len(masked_relevant_groups), wspace=0.09, hspace=0)\n",
    "\n",
    "min_x, max_x = 0.4, 2\n",
    "pallete = sns.color_palette(\"hls\", 11)\n",
    "grey_shadow = '#857e7e'\n",
    "\n",
    "for j, (k, v) in enumerate(masked_relevant_groups.items()):   \n",
    "\n",
    "    column_grid = gridspec.GridSpecFromSubplotSpec(11, 1,\n",
    "                subplot_spec=outer_grid[j], wspace=0, hspace=0.0)\n",
    "\n",
    "    for i in range(0,11):\n",
    "        \n",
    "        threshold = find_threshold(masked_img[k][:3], 0.90)\n",
    "        \n",
    "        ax = plt.Subplot(fig, column_grid[i])  \n",
    "        ax.set(xlim=(min_x, max_x))\n",
    "        if i == 0: ax.set_title(k)\n",
    "        if i == 10:\n",
    "            ax.set_xticks([min_x, threshold, max_x])\n",
    "            ax.set_xticklabels([min_x, round(threshold, 2), max_x])\n",
    "        else:\n",
    "            ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xlabel('')\n",
    "        if j == 0: \n",
    "            ax.set_ylabel(str(i+1), rotation='horizontal')\n",
    "            \n",
    "        ax.axvline(x=threshold, color=grey_shadow, ls=':')\n",
    "        sns.kdeplot(v[i], shade=True, color=pallete[i], # use the same colors as before to identify subjects\n",
    "                    ax=ax)\n",
    "        if 'left_PuL' in k:\n",
    "            sns.kdeplot(masked_img['right_PuL'][i], shade=True, color=grey_shadow, ax=ax)\n",
    "        sns.kdeplot(masked_img['left_global'][i], ls='--', ax=ax, color=grey_shadow)        \n",
    "        \n",
    "        fig.add_subplot(ax)      \n",
    "\n",
    "plt.savefig('results/figs/nuclei.pdf', format='pdf')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/relevant_nuclei.csv', 'r') as f:\n",
    "    relevant_nuclei_key = [ line.split(',')[0] for line in f.readlines() if 'yes' in line ]\n",
    "    \n",
    "relevant_nuclei = ['_'.join(it) for it in itertools.product(['left', 'right'], relevant_nuclei_key)]\n",
    "\n",
    "assert set(relevant_nuclei).issubset(nuclei_masks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "too_small_nuclei = [ k for k, v in nuclei_sizes.items() if v < 50 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "median_control_nuclei = { k: get_median_image(v[:3]) for k, v in masked_img.items() }\n",
    "all_subjects_nuclei = [{ k: get_median_image(v[i]) for k, v in masked_img.items() } ]\n",
    "\n",
    "for k in median_control_nuclei.keys():\n",
    "    if k not in too_small_nuclei:\n",
    "        print(k + ' with size: ' + str(nuclei_sizes[k]))\n",
    "        for i in range(3):\n",
    "            plt.axvline(x=find_threshold(masked_img[k][:3], 0.95))\n",
    "            sns.distplot(masked_img[k][i], kde_kws={\"label\": str(i+1)}, hist=False)\n",
    "        plt.show()\n",
    "        for i in range(3,11): \n",
    "            plt.axvline(x=find_threshold(masked_img[k][:3], 0.95))\n",
    "            sns.distplot(masked_img[k][i], kde_kws={\"label\": str(i+1)}, hist=False)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in median_control_nuclei.keys():\n",
    "    if k not in too_small_nuclei:\n",
    "        plt.plot(range(1,12), np.median(masked_img[k], axis=1)) \n",
    "plt.show()\n",
    "        \n",
    "for k in median_control_nuclei.keys():\n",
    "    if k not in too_small_nuclei:\n",
    "        plt.plot(range(1,12), np.percentile(masked_img[k], q=90, axis=1))\n",
    "        plt.title(k + ' with size: ' + str(nuclei_sizes[k]))\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "cycol = cycle('bgrcmk')\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax = fig.add_subplot(111) \n",
    "\n",
    "for k in median_control_nuclei.keys():\n",
    "    if k not in too_small_nuclei:\n",
    "        threshold = find_threshold(masked_img[k][:3], 0.95)\n",
    "        tmp = np.array([np.sum(img >= threshold) / len(img) for img in masked_img[k]])\n",
    "        keeper = np.sum(tmp[3:] >= 0.15) and (np.sum(tmp[:3] < 0.1) == 3)\n",
    "        ax.plot(range(1,12), tmp, color=next(cycol) if keeper else '0.8', ls='-', label = k if keeper else '')\n",
    "        ax.set_xticks(list(range(1, 12)))\n",
    "        ax.set_xticklabels(list(range(1, 12)))\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "fig.add_subplot(ax)\n",
    "all_axes = fig.get_axes()\n",
    "despine(all_axes)\n",
    "        \n",
    "plt.savefig('results/figs/inflammed_volume_in_thalamic_nuclei.pdf', format='pdf')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, v in median_control_nuclei.items():\n",
    "    if k not in too_small_nuclei:\n",
    "        print(k + ' with size: ' + str(nuclei_sizes[k]))\n",
    "        sns.distplot(v)\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_voxel_data(subjects):\n",
    "    '''\n",
    "    Get the voxel value from a sequence of data images\n",
    "    '''\n",
    "    tmp = [s.reshape(s.size) for s in subjects]\n",
    "    return np.concatenate(tmp)\n",
    "\n",
    "def get_mask_from_dataset(atlas_img, regions):\n",
    "    \n",
    "    atlas_data = atlas_img.get_data()\n",
    "    indexes = [dataset['labels'].index(region) for region in regions]\n",
    "    thresholds = [int(dataset['indices'][ind]) for ind in indexes]\n",
    "    mask = (atlas_data == thresholds[0])\n",
    "    for threshold in thresholds[1:]:\n",
    "        mask = mask | (atlas_data == threshold)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nilearn.datasets import load_mni152_template\n",
    "\n",
    "template = mask_img\n",
    "\n",
    "localizer_tmap_filename = nb.load(list(nucleii_mask_dict.values())[0])\n",
    "resampled_localizer_tmap = resample_to_img(localizer_tmap_filename, template)\n",
    "\n",
    "original_shape = localizer_tmap_filename.shape\n",
    "original_affine = localizer_tmap_filename.get_affine()\n",
    "\n",
    "resampled_shape = resampled_localizer_tmap.shape\n",
    "resampled_affine = resampled_localizer_tmap.get_affine()\n",
    "\n",
    "template_shape = template.shape\n",
    "template_affine = template.get_affine()\n",
    "print(\"\"\"Shape comparison:\n",
    "- Original t-map image shape : {0}\n",
    "- Resampled t-map image shape: {1}\n",
    "- Template image shape       : {2}\n",
    "\"\"\".format(original_shape, resampled_shape, template_shape))\n",
    "\n",
    "print(\"\"\"Affine comparison:\n",
    "- Original t-map image affine :\\n {0}\n",
    "- Resampled t-map image affine:\\n {1}\n",
    "- Template image affine       :\\n {2}\n",
    "\"\"\".format(original_affine, resampled_affine, template_affine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nibabel as nb\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "from nilearn.image import new_img_like\n",
    "from nilearn.masking import apply_mask\n",
    "\n",
    "dataset = datasets.fetch_atlas_aal()\n",
    "atlas_filename = dataset.maps\n",
    "atlas_img = nb.load(atlas_filename)\n",
    "\n",
    "mask = get_mask_from_dataset(atlas_img, ['Thalamus_L', 'Thalamus_R'])\n",
    "mask_img = new_img_like(imgs[0], mask)\n",
    "\n",
    "masked_img = apply_mask(imgs, mask_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(masked_img[0])\n",
    "sns.distplot(masked_img[1])\n",
    "sns.distplot(masked_img[2])\n",
    "#sns.distplot(masked_img[6])\n",
    "sns.distplot(masked_img[8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
